# Large Language Modeling Benchmark

This is a benchmark for a causal language model, 
following LLaMa architecture (e.g. using rotary embeddings) but parameterized to 
contain XXXX paramters, similarly to GPT-2 (1.5B) or GPT-J (6B)

# Running

All hyperparameters are fixed, except for batch size
